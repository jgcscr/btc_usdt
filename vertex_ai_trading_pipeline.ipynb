{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c19e1c",
   "metadata": {},
   "source": [
    "# Vertex AI Trading Pipeline for BTC-USDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4438a6",
   "metadata": {},
   "source": [
    "This notebook outlines the steps to refactor the existing BTC-USDT trading pipeline to leverage Vertex AI services. \n",
    "We will define Kubeflow Pipeline (KFP) components for each stage of the ML workflow and orchestrate them using Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa9fa5",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131379a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component, pipeline, Input, Output, Dataset, Model, Metrics, Artifact\n",
    "\n",
    "# --- GCP Configuration ---\n",
    "# Please fill in these values\n",
    "PROJECT_ID = \"your-gcp-project-id\"  # Replace with your Project ID\n",
    "REGION = \"your-gcp-region\"      # Replace with your GCP Region (e.g., 'us-central1')\n",
    "BUCKET_NAME = \"your-gcs-bucket-name\" # Replace with your GCS Bucket name (e.g., 'my-trading-pipeline-bucket')\n",
    "\n",
    "# --- Pipeline Configuration ---\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/btc_usdt\"\n",
    "PIPELINE_NAME = \"btc-usdt-trading-pipeline\"\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Authenticate (if running locally and not on Vertex AI Workbench)\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)\n",
    "\n",
    "# Add project root to Python path to import custom modules\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd())) # Assumes notebook is in root\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"GCS Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Pipeline Root: {PIPELINE_ROOT}\")\n",
    "print(f\"Python Path includes: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255171df",
   "metadata": {},
   "source": [
    "### 1.1. (Optional) Initial Data Upload to GCS\n",
    "If your raw data (`1m_btcusdt_raw.parquet`) is not already in GCS, you can upload it using the cell below. \n",
    "Ideally, your `fetch_data.py` script should be modified to write directly to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fe5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to gs://{bucket_name}/{destination_blob_name}.\"\n",
    "    )\n",
    "\n",
    "# Example: Upload raw data if it's local\n",
    "LOCAL_RAW_DATA_PATH = 'data/1m_btcusdt_raw.parquet' # Adjust if your local path is different\n",
    "GCS_RAW_DATA_PATH = 'data/raw/1m_btcusdt_raw.parquet' # Desired path in GCS\n",
    "\n",
    "if os.path.exists(LOCAL_RAW_DATA_PATH):\n",
    "    # upload_to_gcs(BUCKET_NAME, LOCAL_RAW_DATA_PATH, GCS_RAW_DATA_PATH)\n",
    "    print(f\"To upload, uncomment and run: upload_to_gcs({BUCKET_NAME}, {LOCAL_RAW_DATA_PATH}, {GCS_RAW_DATA_PATH})\")\n",
    "else:\n",
    "    print(f\"Local raw data file not found at {LOCAL_RAW_DATA_PATH}. Ensure it's available or already in GCS at gs://{BUCKET_NAME}/{GCS_RAW_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4727efb",
   "metadata": {},
   "source": [
    "## 2. Define KFP Components\n",
    "We will now define KFP components for each step of the pipeline. These components will encapsulate the logic from your existing Python scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58d77f",
   "metadata": {},
   "source": [
    "### 2.1. Data Ingestion and Preparation Component\n",
    "This component will be responsible for fetching the latest data (if applicable) and preparing it for feature engineering. It should output a GCS path to the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\", # Choose an appropriate base image\n",
    "    packages_to_install=[\"google-cloud-storage\", \"pandas\", \"pyarrow\"] # Add other necessary packages\n",
    ")\n",
    "def data_ingestion_component(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    gcs_raw_data_uri: str, # Input: gs://bucket/path/to/raw_data.parquet\n",
    "    output_prepared_data_uri: Output[Dataset] # Output: gs://bucket/path/to/prepared_data.parquet\n",
    "):\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "\n",
    "    # This is a placeholder. You'll need to adapt your fetch_data.py logic here.\n",
    "    # For now, let's assume raw data is already in GCS and we just copy it or do minimal prep.\n",
    "    \n",
    "    # Example: Read from gcs_raw_data_uri, process, and write to output_prepared_data_uri.path\n",
    "    df = pd.read_parquet(gcs_raw_data_uri)\n",
    "    \n",
    "    # Perform any initial preparation if needed\n",
    "    # df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Ensure output directory exists (KFP handles this for Output[Dataset])\n",
    "    # os.makedirs(os.path.dirname(output_prepared_data_uri.path), exist_ok=True) # Not needed for GCS URI\n",
    "    df.to_parquet(output_prepared_data_uri.path, index=False)\n",
    "    output_prepared_data_uri.metadata[\"gcs_path\"] = output_prepared_data_uri.path\n",
    "    print(f\"Prepared data saved to: {output_prepared_data_uri.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabbfda",
   "metadata": {},
   "source": [
    "### 2.2. Feature Engineering Component\n",
    "This component will take the prepared data, compute features, and save the enriched dataset to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efdd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"pandas\", \"pyarrow\", \"numpy\", \"technicalindicators\"] # Add your project's specific dependencies for compute_features.py\n",
    "    # If your project has many local modules, consider building a custom container image.\n",
    ")\n",
    "def feature_engineering_component(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    prepared_data: Input[Dataset],\n",
    "    output_enriched_data_uri: Output[Dataset] # Output: gs://bucket/path/to/enriched_data.parquet\n",
    "):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    # You will need to import or include your feature engineering logic here.\n",
    "    # from btc_usdt_pipeline.features.compute_features import FeatureEngineer # Example\n",
    "    \n",
    "    print(f\"Prepared data URI: {prepared_data.uri}\")\n",
    "    df = pd.read_parquet(prepared_data.uri)\n",
    "    \n",
    "    # Placeholder for actual feature engineering logic from compute_features.py\n",
    "    # This is highly dependent on your existing 'compute_features.py' script.\n",
    "    # You might need to adapt it to be callable as a function.\n",
    "    # Example:\n",
    "    # feature_engineer = FeatureEngineer(df.copy())\n",
    "    # enriched_df = feature_engineer.compute_all_features()\n",
    "    \n",
    "    # For now, let's assume a simple transformation or pass-through\n",
    "    enriched_df = df # Replace with actual feature computation\n",
    "    enriched_df['example_feature'] = 1.0 \n",
    "    \n",
    "    enriched_df.to_parquet(output_enriched_data_uri.path, index=False)\n",
    "    output_enriched_data_uri.metadata[\"gcs_path\"] = output_enriched_data_uri.path\n",
    "    print(f\"Enriched data saved to: {output_enriched_data_uri.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248d813",
   "metadata": {},
   "source": [
    "### 2.3. Model Training Component (Custom Training Job)\n",
    "This component will launch a Vertex AI Custom Training Job. Your `train.py` script will need to be packaged (e.g., in a Docker container if it has complex dependencies or needs specific Python versions/libraries not easily installed via `packages_to_install`) or adapted to run in a standard Vertex AI training container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ae90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This component definition is more complex as it typically involves creating a training script \n",
    "# and potentially a Dockerfile if you're using a custom container.\n",
    "\n",
    "# Step 1: Create your training script (e.g., 'training_script.py')\n",
    "# This script should:\n",
    "#   - Accept command-line arguments for hyperparameters, data paths (GCS URIs from previous steps).\n",
    "#   - Load data from GCS.\n",
    "#   - Train the model.\n",
    "#   - Save the trained model to a GCS path (provided by Vertex AI, e.g., AIP_MODEL_DIR).\n",
    "#   - Optionally, save metrics.\n",
    "\n",
    "TRAINING_SCRIPT_CONTENT = '''\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier # Example model\n",
    "import joblib\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--enriched_data_path', type=str, required=True)\n",
    "parser.add_argument('--n_estimators', type=int, default=100)\n",
    "parser.add_argument('--target_column', type=str, default='target_move') # Example target\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"Loading data from: {args.enriched_data_path}\")\n",
    "df = pd.read_parquet(args.enriched_data_path)\n",
    "\n",
    "# Simple data prep for example\n",
    "df = df.dropna(subset=[args.target_column]) # Ensure target is not NaN\n",
    "features = [col for col in df.columns if col not in [args.target_column, 'open_time', 'close_time']] # Adjust features\n",
    "X = df[features].fillna(0) # Simple NaN fill, improve this\n",
    "y = df[args.target_column]\n",
    "\n",
    "if X.empty or y.empty:\n",
    "    print(\"No data to train after preprocessing.\")\n",
    "    # Create a dummy model file if required by Vertex AI\n",
    "    # model_filename = 'dummy_model.joblib'\n",
    "    # joblib.dump({}, os.path.join(os.environ.get('AIP_MODEL_DIR', '.'), model_filename))\n",
    "    # print(f\"Dummy model saved to {os.path.join(os.environ.get('AIP_MODEL_DIR', '.'), model_filename)}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Training RandomForestClassifier with n_estimators={args.n_estimators}\")\n",
    "model = RandomForestClassifier(n_estimators=args.n_estimators, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "model_filename = 'model.joblib'\n",
    "# AIP_MODEL_DIR is an environment variable set by Vertex AI Training for the GCS location to save the model\n",
    "model_save_path = os.path.join(os.environ.get('AIP_MODEL_DIR', '.'), model_filename)\n",
    "joblib.dump(model, model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "'''\n",
    "\n",
    "with open(\"training_script.py\", \"w\") as f:\n",
    "    f.write(TRAINING_SCRIPT_CONTENT)\n",
    "\n",
    "print(\"Created training_script.py. You'll need to upload this to GCS or include in a custom container.\")\n",
    "\n",
    "# Upload training script to GCS (if not using a custom container with the script baked in)\n",
    "GCS_TRAINING_SCRIPT_PATH = f\"gs://{BUCKET_NAME}/training_scripts/trading_pipeline/training_script.py\"\n",
    "try:\n",
    "    upload_to_gcs(BUCKET_NAME, \"training_script.py\", GCS_TRAINING_SCRIPT_PATH.replace(f\"gs://{BUCKET_NAME}/\", \"\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading training script: {e}. Ensure GCS bucket is configured and accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163c366",
   "metadata": {},
   "source": [
    "### 2.4. Model Evaluation/Backtesting Component\n",
    "This component will take the trained model and the enriched data (or a specific test set) to perform backtesting and generate evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d11c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"google-cloud-aiplatform\", \"pandas\", \"pyarrow\", \"scikit-learn\", \"joblib\"] # Add packages for backtest.py\n",
    ")\n",
    "def model_evaluation_component(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    enriched_data: Input[Dataset], # Or a dedicated test dataset\n",
    "    trained_model: Input[Model], # Model from the training step\n",
    "    gcs_metrics_path: str, # GCS path to save metrics JSON\n",
    "    evaluation_metrics: Output[Metrics],\n",
    "    model_performance: Output[Artifact] # Could be a visualization or detailed report\n",
    "):\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import json\n",
    "    import os\n",
    "    from sklearn.metrics import classification_report # Example metrics\n",
    "    from google.cloud import storage\n",
    "\n",
    "    print(f\"Model URI: {trained_model.uri}\")\n",
    "    # Model URI from custom training job is a GCS directory. Model file is inside.\n",
    "    # We need to find the actual model file (e.g., model.joblib)\n",
    "    model_directory = trained_model.uri.replace('gs://', '') # Remove gs:// prefix for GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name_model = model_directory.split('/')[0]\n",
    "    model_path_prefix = '/'.join(model_directory.split('/')[1:])\n",
    "    \n",
    "    model_blob_path = None\n",
    "    blobs = storage_client.list_blobs(bucket_name_model, prefix=model_path_prefix)\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('model.joblib'): # Or your specific model file name\n",
    "            model_blob_path = f\"gs://{bucket_name_model}/{blob.name}\"\n",
    "            break\n",
    "    \n",
    "    if not model_blob_path:\n",
    "        raise FileNotFoundError(f\"Model file (e.g., model.joblib) not found in {trained_model.uri}\")\n",
    "\n",
    "    print(f\"Actual model file path: {model_blob_path}\")\n",
    "    # Download the model to a local temp file to load it\n",
    "    local_model_file = 'downloaded_model.joblib'\n",
    "    blob = storage.Blob.from_string(model_blob_path, client=storage_client)\n",
    "    blob.download_to_filename(local_model_file)\n",
    "    model = joblib.load(local_model_file)\n",
    "\n",
    "    print(f\"Enriched data URI: {enriched_data.uri}\")\n",
    "    df = pd.read_parquet(enriched_data.uri)\n",
    "\n",
    "    # Placeholder for backtesting logic from backtest.py\n",
    "    # This is highly dependent on your 'backtest.py' and 'model_metrics.py'\n",
    "    # Example using scikit-learn metrics:\n",
    "    target_column = 'target_move' # Example target\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    features = [col for col in df.columns if col not in [target_column, 'open_time', 'close_time']]\n",
    "    X_test = df[features].fillna(0)\n",
    "    y_test = df[target_column]\n",
    "\n",
    "    if X_test.empty or y_test.empty:\n",
    "        print(\"No data for evaluation.\")\n",
    "        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        metrics = {\n",
    "            'accuracy': report.get('accuracy', 0.0),\n",
    "            'precision': report.get('weighted avg', {}).get('precision', 0.0),\n",
    "            'recall': report.get('weighted avg', {}).get('recall', 0.0),\n",
    "            'f1_score': report.get('weighted avg', {}).get('f1-score', 0.0)\n",
    "        }\n",
    "\n",
    "    print(f\"Evaluation Metrics: {metrics}\")\n",
    "    evaluation_metrics.log_metric(\"accuracy\", metrics['accuracy'])\n",
    "    evaluation_metrics.log_metric(\"f1_score\", metrics['f1_score'])\n",
    "\n",
    "    # Save metrics to GCS\n",
    "    # The gcs_metrics_path should be like 'gs://bucket/path/to/metrics.json'\n",
    "    # KFP Output[Metrics] also stores them, but saving a separate JSON can be useful.\n",
    "    # Ensure the path is a blob path, not a directory\n",
    "    if not gcs_metrics_path.startswith(\"gs://\"):\n",
    "        actual_gcs_metrics_path = f\"gs://{bucket_name}/{gcs_metrics_path.lstrip(/)}\"\n",
    "    else:\n",
    "        actual_gcs_metrics_path = gcs_metrics_path\n",
    "        \n",
    "    bucket_name_metrics = actual_gcs_metrics_path.split('/')[2]\n",
    "    blob_name_metrics = '/'.join(actual_gcs_metrics_path.split('/')[3:])\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name_metrics)\n",
    "    blob = bucket.blob(blob_name_metrics)\n",
    "    blob.upload_from_string(\n",
    "        data=json.dumps(metrics, indent=4),\n",
    "        content_type='application/json'\n",
    "    )\n",
    "    print(f\"Metrics saved to {actual_gcs_metrics_path}\")\n",
    "\n",
    "    # Create a simple artifact for model performance (e.g., a markdown file)\n",
    "    with open(model_performance.path, \"w\") as f:\n",
    "        f.write(f\"# Model Performance Report\n",
    "\")\n",
    "        f.write(f\"Accuracy: {metrics['accuracy']:.4f}\n",
    "\")\n",
    "        f.write(f\"F1-score: {metrics['f1_score']:.4f}\n",
    "\")\n",
    "        f.write(f\"Full metrics available at: {actual_gcs_metrics_path}\")\n",
    "    model_performance.metadata[\"description\"] = \"Basic model performance report.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00ca94",
   "metadata": {},
   "source": [
    "## 3. Define the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a972c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"An E2E pipeline for BTC-USDT trading: data ingestion, feature engineering, training, and evaluation.\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def trading_pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    gcs_raw_data_uri: str = f\"gs://{BUCKET_NAME}/data/raw/1m_btcusdt_raw.parquet\", # Default path\n",
    "    gcs_prepared_data_base_path: str = f\"gs://{BUCKET_NAME}/data/prepared/\",\n",
    "    gcs_enriched_data_base_path: str = f\"gs://{BUCKET_NAME}/data/enriched/\",\n",
    "    gcs_metrics_base_path: str = f\"gs://{BUCKET_NAME}/evaluation_metrics/\",\n",
    "    training_script_gcs_uri: str = GCS_TRAINING_SCRIPT_PATH,\n",
    "    training_container_uri: str = \"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.0-23:latest\", # Pre-built container for scikit-learn\n",
    "    model_display_name: str = \"btc-usdt-trading-model\",\n",
    "    n_estimators_train: int = 100 # Hyperparameter for training\n",
    "):\n",
    "    # Data Ingestion\n",
    "    data_ingestion_task = data_ingestion_component(\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        gcs_raw_data_uri=gcs_raw_data_uri\n",
    "        # output_prepared_data_uri will be named by KFP, e.g., {gcs_prepared_data_base_path}pipeline_name/run_id/task_name/output_artifact_name\n",
    "    ).set_display_name(\"Data Ingestion\")\n",
    "\n",
    "    # Feature Engineering\n",
    "    feature_engineering_task = feature_engineering_component(\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        prepared_data=data_ingestion_task.outputs[\"output_prepared_data_uri\"]\n",
    "    ).set_display_name(\"Feature Engineering\")\n",
    "\n",
    "    # Model Training (using CustomTrainingJobOp for more control or a custom component that wraps it)\n",
    "    # For simplicity, we'll use a pre-built container and pass the script.\n",
    "    # The model output will be a GCS URI to the directory containing the model artifacts.\n",
    "    # This requires the training_script.py to be accessible, e.g., in GCS.\n",
    "    from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "\n",
    "    # Define where the model artifacts will be stored by the training job\n",
    "    # Vertex AI will create a unique subdirectory here for each training job run\n",
    "    model_output_directory = f\"gs://{bucket_name}/trained_models/{PIPELINE_NAME}/\"\n",
    "\n",
    "    custom_train_job_task = CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        display_name=\"btc-usdt-custom-training\",\n",
    "        script_path=training_script_gcs_uri, # GCS path to your training script\n",
    "        container_uri=training_container_uri, # Or your custom container\n",
    "        requirements=[], # Add if your script needs packages not in the container\n",
    "        model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\", # For deploying scikit-learn models\n",
    "        # Pass arguments to your training script\n",
    "        args=[\n",
    "            f\"--enriched_data_path={feature_engineering_task.outputs['output_enriched_data_uri'].uri}\",\n",
    "            f\"--n_estimators={n_estimators_train}\",\n",
    "            # Add other args your training script expects\n",
    "        ],\n",
    "        # Define the GCS directory where the model should be saved by the training script (AIP_MODEL_DIR)\n",
    "        base_output_directory=model_output_directory,\n",
    "        # Ensure this task runs after feature engineering\n",
    "        # KFP infers this from data passing, but explicit dependency can be set if needed\n",
    "    ).set_display_name(\"Model Training\")\n",
    "    # custom_train_job_task.after(feature_engineering_task) # Usually not needed if outputs are passed\n",
    "\n",
    "    # Model Evaluation\n",
    "    # The model artifact from CustomTrainingJobOp is a GCS path to the directory.\n",
    "    # The evaluation component needs to know how to load the model from this directory.\n",
    "    evaluation_task = model_evaluation_component(\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        enriched_data=feature_engineering_task.outputs[\"output_enriched_data_uri\"],\n",
    "        trained_model=custom_train_job_task.outputs[\"model\"], # This output refers to the GCS directory\n",
    "        gcs_metrics_path=f\"{gcs_metrics_base_path}{PIPELINE_NAME}/{{dsl.PIPELINE_JOB_ID}}/evaluation_metrics.json\"\n",
    "    ).set_display_name(\"Model Evaluation\")\n",
    "\n",
    "    # (Optional) Model Upload and Deployment\n",
    "    # This section can be added if you want to automatically register and deploy the model.\n",
    "    # For now, we'll focus on training and evaluation.\n",
    "\n",
    "    # Example: Upload model to Vertex AI Model Registry\n",
    "    # model_upload_op = ModelUploadOp(\n",
    "    #     project=project_id,\n",
    "    #     display_name=model_display_name,\n",
    "    #     artifact_uri=custom_train_job_task.outputs[\"model\"].uri, # GCS path to model directory\n",
    "    #     serving_container_image_uri=training_container_uri, # Or a specific prediction container\n",
    "    #     # depends_on evaluation_task passing some threshold\n",
    "    # )\n",
    "    # model_upload_op.after(evaluation_task)\n",
    "\n",
    "    # Example: Create Endpoint and Deploy Model\n",
    "    # endpoint_create_op = EndpointCreateOp(\n",
    "    #     project=project_id,\n",
    "    #     display_name=f\"{model_display_name}-endpoint\"\n",
    "    # )\n",
    "    # endpoint_create_op.after(model_upload_op)\n",
    "    # \n",
    "    # model_deploy_op = ModelDeployOp(\n",
    "    #     project=project_id,\n",
    "    #     endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "    #     model=model_upload_op.outputs[\"model\"],\n",
    "    #     deployed_model_display_name=model_display_name,\n",
    "    #     dedicated_resources_machine_type=\"n1-standard-2\", # Choose machine type\n",
    "    #     dedicated_resources_min_replica_count=1,\n",
    "    #     dedicated_resources_max_replica_count=1,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589044ad",
   "metadata": {},
   "source": [
    "## 4. Compile and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON_SPEC_PATH = \"btc_usdt_trading_pipeline.json\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=trading_pipeline,\n",
    "    package_path=PIPELINE_JSON_SPEC_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Pipeline compiled to {PIPELINE_JSON_SPEC_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05df317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "\n",
    "try:\n",
    "    pipeline_job = aiplatform.PipelineJob(\n",
    "        display_name=PIPELINE_NAME,\n",
    "        template_path=PIPELINE_JSON_SPEC_PATH,\n",
    "        pipeline_root=PIPELINE_ROOT, # Redundant if already in pipeline decorator, but good for clarity\n",
    "        # Pass pipeline parameters if they differ from defaults\n",
    "        parameter_values={\n",
    "            'project_id': PROJECT_ID,\n",
    "            'bucket_name': BUCKET_NAME,\n",
    "            # 'gcs_raw_data_uri': 'gs://my-other-bucket/data/raw/1m_btcusdt_raw.parquet', # Example override\n",
    "            'n_estimators_train': 150 # Example override of a training hyperparameter\n",
    "        },\n",
    "        enable_caching=True # Set to False to disable caching for a run\n",
    "    )\n",
    "\n",
    "    pipeline_job.submit()\n",
    "    print(f\"Pipeline job submitted. View in Vertex AI Pipelines: {pipeline_job._dashboard_uri()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error submitting pipeline job: {e}\")\n",
    "    print(\"Please ensure your PROJECT_ID, REGION, and BUCKET_NAME are correctly set, \")\n",
    "    print(\"and that the GCS bucket exists and the Vertex AI API is enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cccd3c",
   "metadata": {},
   "source": [
    "## 5. Next Steps and Refinements\n",
    "\n",
    "1.  **Implement Component Logic:** Replace placeholder logic in `data_ingestion_component`, `feature_engineering_component`, and `model_evaluation_component` with actual code adapted from your existing scripts (`fetch_data.py`, `compute_features.py`, `backtest.py`, `model_metrics.py`).\n",
    "2.  **Refine Training Script:** Adapt your `train.py` to be the `training_script.py` used by the `CustomTrainingJobOp`. Ensure it handles GCS paths for input data and model output (using `AIP_MODEL_DIR`).\n",
    "3.  **Custom Containers (If Needed):** If your components or training script have complex dependencies not easily installed via `packages_to_install` or `requirements` in `CustomTrainingJobOp`, you'll need to build custom Docker container images and push them to Google Container Registry (GCR) or Artifact Registry. Then, update the `base_image` for components or `container_uri` for the training job.\n",
    "4.  **Configuration Management:** Integrate your `ConfigManager` or a similar approach for handling configurations (e.g., feature lists, model parameters) within the components, potentially by passing config file paths (in GCS) or specific config values as parameters.\n",
    "5.  **Error Handling and Logging:** Add robust error handling and logging within each component.\n",
    "6.  **Model Registry and Deployment:** Implement the commented-out sections for `ModelUploadOp`, `EndpointCreateOp`, and `ModelDeployOp` if you want to automate model registration and deployment to a Vertex AI Endpoint.\n",
    "7.  **Hyperparameter Tuning:** Vertex AI offers HyperparameterTuningJob which can be integrated into your pipeline before the main training step.\n",
    "8.  **Monitoring:** Set up Vertex AI Model Monitoring for deployed models.\n",
    "9.  **Testing:** Develop tests for your KFP components."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
