{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c19e1c",
   "metadata": {},
   "source": [
    "# Vertex AI Trading Pipeline for BTC-USDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4438a6",
   "metadata": {},
   "source": [
    "This notebook outlines the steps to refactor the existing BTC-USDT trading pipeline to leverage Vertex AI services. \n",
    "We will define Kubeflow Pipeline (KFP) components for each stage of the ML workflow and orchestrate them using Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa9fa5",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131379a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: jc-financial-459605\n",
      "Region: us-east4\n",
      "GCS Bucket: jc-financial-459605-raw-data-bucket\n",
      "Pipeline Root: gs://jc-financial-459605-raw-data-bucket/pipeline_root/btc_usdt\n",
      "Python Path includes: /workspaces/btc_usdt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component, pipeline, Input, Output, Dataset, Model, Metrics, Artifact\n",
    "\n",
    "# --- GCP Configuration ---\n",
    "# Please fill in these values\n",
    "PROJECT_ID = \"jc-financial-459605\"  # Replace with your Project ID\n",
    "REGION = \"us-east4\"      # Replace with your GCP Region (e.g., 'us-central1')\n",
    "BUCKET_NAME = \"jc-financial-459605-raw-data-bucket\" # Replace with your GCS Bucket name (e.g., 'my-trading-pipeline-bucket')\n",
    "\n",
    "# --- Pipeline Configuration ---\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/btc_usdt\"\n",
    "PIPELINE_NAME = \"btc-usdt-trading-pipeline\"\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Authenticate (if running locally and not on Vertex AI Workbench)\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)\n",
    "\n",
    "# Add project root to Python path to import custom modules\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd())) # Assumes notebook is in root\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"GCS Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Pipeline Root: {PIPELINE_ROOT}\")\n",
    "print(f\"Python Path includes: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255171df",
   "metadata": {},
   "source": [
    "### 1.1. (Optional) Initial Data Upload to GCS\n",
    "If your raw data (`1m_btcusdt_raw.parquet`) is not already in GCS, you can upload it using the cell below. \n",
    "Ideally, your `fetch_data.py` script should be modified to write directly to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fe5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to gs://{bucket_name}/{destination_blob_name}.\"\n",
    "    )\n",
    "\n",
    "# Example: Upload raw data if it's local\n",
    "LOCAL_RAW_DATA_PATH = 'data/1m_btcusdt_raw.parquet' # Adjust if your local path is different\n",
    "GCS_RAW_DATA_PATH = 'data/raw/1m_btcusdt_raw.parquet' # Desired path in GCS\n",
    "\n",
    "if os.path.exists(LOCAL_RAW_DATA_PATH):\n",
    "    # upload_to_gcs(BUCKET_NAME, LOCAL_RAW_DATA_PATH, GCS_RAW_DATA_PATH)\n",
    "    print(f\"To upload, uncomment and run: upload_to_gcs({BUCKET_NAME}, {LOCAL_RAW_DATA_PATH}, {GCS_RAW_DATA_PATH})\")\n",
    "else:\n",
    "    print(f\"Local raw data file not found at {LOCAL_RAW_DATA_PATH}. Ensure it's available or already in GCS at gs://{BUCKET_NAME}/{GCS_RAW_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4727efb",
   "metadata": {},
   "source": [
    "## 2. Define KFP Components\n",
    "We will now define KFP components for each step of the pipeline. These components will encapsulate the logic from your existing Python scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58d77f",
   "metadata": {},
   "source": [
    "### 2.1. Data Ingestion and Preparation Component\n",
    "This component will be responsible for fetching the latest data (if applicable) and preparing it for feature engineering. It should output a GCS path to the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\", # Choose an appropriate base image\n",
    "    packages_to_install=[\"google-cloud-storage\", \"pandas\", \"pyarrow\"] # Add other necessary packages\n",
    ")\n",
    "def data_ingestion_component(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    gcs_raw_data_uri: str, # Input: gs://bucket/path/to/raw_data.parquet\n",
    "    output_prepared_data_uri: Output[Dataset] # Output: gs://bucket/path/to/prepared_data.parquet\n",
    "):\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "\n",
    "    # This is a placeholder. You'll need to adapt your fetch_data.py logic here.\n",
    "    # For now, let's assume raw data is already in GCS and we just copy it or do minimal prep.\n",
    "    \n",
    "    # Example: Read from gcs_raw_data_uri, process, and write to output_prepared_data_uri.path\n",
    "    df = pd.read_parquet(gcs_raw_data_uri)\n",
    "    \n",
    "    # Perform any initial preparation if needed\n",
    "    # df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Ensure output directory exists (KFP handles this for Output[Dataset])\n",
    "    # os.makedirs(os.path.dirname(output_prepared_data_uri.path), exist_ok=True) # Not needed for GCS URI\n",
    "    df.to_parquet(output_prepared_data_uri.path, index=False)\n",
    "    output_prepared_data_uri.metadata[\"gcs_path\"] = output_prepared_data_uri.path\n",
    "    print(f\"Prepared data saved to: {output_prepared_data_uri.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabbfda",
   "metadata": {},
   "source": [
    "### 2.2. Feature Engineering Component\n",
    "This component will take the prepared data, compute features, and save the enriched dataset to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efdd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"pandas\", \"pyarrow\", \"numpy\", \"technicalindicators\"] # Add your project's specific dependencies for compute_features.py\n",
    "    # If your project has many local modules, consider building a custom container image.\n",
    ")\n",
    "def feature_engineering_component(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    prepared_data: Input[Dataset],\n",
    "    output_enriched_data_uri: Output[Dataset] # Output: gs://bucket/path/to/enriched_data.parquet\n",
    "):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    # You will need to import or include your feature engineering logic here.\n",
    "    # from btc_usdt_pipeline.features.compute_features import FeatureEngineer # Example\n",
    "    \n",
    "    print(f\"Prepared data URI: {prepared_data.uri}\")\n",
    "    df = pd.read_parquet(prepared_data.uri)\n",
    "    \n",
    "    # Placeholder for actual feature engineering logic from compute_features.py\n",
    "    # This is highly dependent on your existing 'compute_features.py' script.\n",
    "    # You might need to adapt it to be callable as a function.\n",
    "    # Example:\n",
    "    # feature_engineer = FeatureEngineer(df.copy())\n",
    "    # enriched_df = feature_engineer.compute_all_features()\n",
    "    \n",
    "    # For now, let's assume a simple transformation or pass-through\n",
    "    enriched_df = df # Replace with actual feature computation\n",
    "    enriched_df['example_feature'] = 1.0 \n",
    "    \n",
    "    enriched_df.to_parquet(output_enriched_data_uri.path, index=False)\n",
    "    output_enriched_data_uri.metadata[\"gcs_path\"] = output_enriched_data_uri.path\n",
    "    print(f\"Enriched data saved to: {output_enriched_data_uri.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248d813",
   "metadata": {},
   "source": [
    "### 2.3. Model Training Component (Custom Training Job)\n",
    "This component will launch a Vertex AI Custom Training Job. Your `train.py` script will need to be packaged (e.g., in a Docker container if it has complex dependencies or needs specific Python versions/libraries not easily installed via `packages_to_install`) or adapted to run in a standard Vertex AI training container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ae90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This component definition is more complex as it typically involves creating a training script \n",
    "# and potentially a Dockerfile if you're using a custom container.\n",
    "\n",
    "# Step 1: Create your training script (e.g., 'training_script.py')\n",
    "# This script should:\n",
    "#   - Accept command-line arguments for hyperparameters, data paths (GCS URIs from previous steps).\n",
    "#   - Load data from GCS.\n",
    "#   - Train the model.\n",
    "#   - Save the trained model to a GCS path (provided by Vertex AI, e.g., AIP_MODEL_DIR).\n",
    "#   - Optionally, save metrics.\n",
    "\n",
    "TRAINING_SCRIPT_CONTENT = '''\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier # Example model\n",
    "import joblib\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--enriched_data_path', type=str, required=True)\n",
    "parser.add_argument('--n_estimators', type=int, default=100)\n",
    "parser.add_argument('--target_column', type=str, default='target_move') # Example target\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"Loading data from: {args.enriched_data_path}\")\n",
    "df = pd.read_parquet(args.enriched_data_path)\n",
    "\n",
    "# Simple data prep for example\n",
    "df = df.dropna(subset=[args.target_column]) # Ensure target is not NaN\n",
    "features = [col for col in df.columns if col not in [args.target_column, 'open_time', 'close_time']] # Adjust features\n",
    "X = df[features].fillna(0) # Simple NaN fill, improve this\n",
    "y = df[args.target_column]\n",
    "\n",
    "if X.empty or y.empty:\n",
    "    print(\"No data to train after preprocessing.\")\n",
    "    # Create a dummy model file if required by Vertex AI\n",
    "    # model_filename = 'dummy_model.joblib'\n",
    "    # joblib.dump({}, os.path.join(os.environ.get('AIP_MODEL_DIR', '.'), model_filename))\n",
    "    # print(f\"Dummy model saved to {os.path.join(os.environ.get('AIP_MODEL_DIR', '.'), model_filename)}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Training RandomForestClassifier with n_estimators={args.n_estimators}\")\n",
    "model = RandomForestClassifier(n_estimators=args.n_estimators, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "model_filename = 'model.joblib'\n",
    "# AIP_MODEL_DIR is an environment variable set by Vertex AI Training for the GCS location to save the model\n",
    "model_save_path = os.path.join(os.environ.get('AIP_MODEL_DIR', '.'), model_filename)\n",
    "joblib.dump(model, model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "'''\n",
    "\n",
    "with open(\"training_script.py\", \"w\") as f:\n",
    "    f.write(TRAINING_SCRIPT_CONTENT)\n",
    "\n",
    "print(\"Created training_script.py. You'll need to upload this to GCS or include in a custom container.\")\n",
    "\n",
    "# Upload training script to GCS (if not using a custom container with the script baked in)\n",
    "GCS_TRAINING_SCRIPT_PATH = f\"gs://{BUCKET_NAME}/training_scripts/trading_pipeline/training_script.py\"\n",
    "try:\n",
    "    upload_to_gcs(BUCKET_NAME, \"training_script.py\", GCS_TRAINING_SCRIPT_PATH.replace(f\"gs://{BUCKET_NAME}/\", \"\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading training script: {e}. Ensure GCS bucket is configured and accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163c366",
   "metadata": {},
   "source": [
    "### 2.4. Model Evaluation/Backtesting Component\n",
    "This component will take the trained model and the enriched data (or a specific test set) to perform backtesting and generate evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d11c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"google-cloud-aiplatform\", \"pandas\", \"pyarrow\", \"scikit-learn\", \"joblib\"] # Add packages for backtest.py\n",
    ")\n",
    "def model_evaluation_component(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    enriched_data: Input[Dataset], # Or a dedicated test dataset\n",
    "    trained_model: Input[Model], # Model from the training step\n",
    "    gcs_metrics_path: str, # GCS path to save metrics JSON\n",
    "    evaluation_metrics: Output[Metrics],\n",
    "    model_performance: Output[Artifact] # Could be a visualization or detailed report\n",
    "):\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import json\n",
    "    import os\n",
    "    from sklearn.metrics import classification_report # Example metrics\n",
    "    from google.cloud import storage\n",
    "\n",
    "    print(f\"Model URI: {trained_model.uri}\")\n",
    "    # Model URI from custom training job is a GCS directory. Model file is inside.\n",
    "    # We need to find the actual model file (e.g., model.joblib)\n",
    "    model_directory = trained_model.uri.replace('gs://', '') # Remove gs:// prefix for GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name_model = model_directory.split('/')[0]\n",
    "    model_path_prefix = '/'.join(model_directory.split('/')[1:])\n",
    "    \n",
    "    model_blob_path = None\n",
    "    blobs = storage_client.list_blobs(bucket_name_model, prefix=model_path_prefix)\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('model.joblib'): # Or your specific model file name\n",
    "            model_blob_path = f\"gs://{bucket_name_model}/{blob.name}\"\n",
    "            break\n",
    "    \n",
    "    if not model_blob_path:\n",
    "        raise FileNotFoundError(f\"Model file (e.g., model.joblib) not found in {trained_model.uri}\")\n",
    "\n",
    "    print(f\"Actual model file path: {model_blob_path}\")\n",
    "    # Download the model to a local temp file to load it\n",
    "    local_model_file = 'downloaded_model.joblib'\n",
    "    blob = storage.Blob.from_string(model_blob_path, client=storage_client)\n",
    "    blob.download_to_filename(local_model_file)\n",
    "    model = joblib.load(local_model_file)\n",
    "\n",
    "    print(f\"Enriched data URI: {enriched_data.uri}\")\n",
    "    df = pd.read_parquet(enriched_data.uri)\n",
    "\n",
    "    # Placeholder for backtesting logic from backtest.py\n",
    "    # This is highly dependent on your 'backtest.py' and 'model_metrics.py'\n",
    "    # Example using scikit-learn metrics:\n",
    "    target_column = 'target_move' # Example target\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    features = [col for col in df.columns if col not in [target_column, 'open_time', 'close_time']]\n",
    "    X_test = df[features].fillna(0)\n",
    "    y_test = df[target_column]\n",
    "\n",
    "    if X_test.empty or y_test.empty:\n",
    "        print(\"No data for evaluation.\")\n",
    "        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        metrics = {\n",
    "            'accuracy': report.get('accuracy', 0.0),\n",
    "            'precision': report.get('weighted avg', {}).get('precision', 0.0),\n",
    "            'recall': report.get('weighted avg', {}).get('recall', 0.0),\n",
    "            'f1_score': report.get('weighted avg', {}).get('f1-score', 0.0)\n",
    "        }\n",
    "\n",
    "    print(f\"Evaluation Metrics: {metrics}\")\n",
    "    evaluation_metrics.log_metric(\"accuracy\", metrics['accuracy'])\n",
    "    evaluation_metrics.log_metric(\"f1_score\", metrics['f1_score'])\n",
    "\n",
    "    # Save metrics to GCS\n",
    "    # The gcs_metrics_path should be like 'gs://bucket/path/to/metrics.json'\n",
    "    # KFP Output[Metrics] also stores them, but saving a separate JSON can be useful.\n",
    "    # Ensure the path is a blob path, not a directory\n",
    "    if not gcs_metrics_path.startswith(\"gs://\"):\n",
    "        actual_gcs_metrics_path = f\"gs://{bucket_name}/{gcs_metrics_path.lstrip(/)}\"\n",
    "    else:\n",
    "        actual_gcs_metrics_path = gcs_metrics_path\n",
    "        \n",
    "    bucket_name_metrics = actual_gcs_metrics_path.split('/')[2]\n",
    "    blob_name_metrics = '/'.join(actual_gcs_metrics_path.split('/')[3:])\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name_metrics)\n",
    "    blob = bucket.blob(blob_name_metrics)\n",
    "    blob.upload_from_string(\n",
    "        data=json.dumps(metrics, indent=4),\n",
    "        content_type='application/json'\n",
    "    )\n",
    "    print(f\"Metrics saved to {actual_gcs_metrics_path}\")\n",
    "\n",
    "    # Create a simple artifact for model performance (e.g., a markdown file)\n",
    "    with open(model_performance.path, \"w\") as f:\n",
    "        f.write(f\"# Model Performance Report\n",
    "\")\n",
    "        f.write(f\"Accuracy: {metrics['accuracy']:.4f}\n",
    "\")\n",
    "        f.write(f\"F1-score: {metrics['f1_score']:.4f}\n",
    "\")\n",
    "        f.write(f\"Full metrics available at: {actual_gcs_metrics_path}\")\n",
    "    model_performance.metadata[\"description\"] = \"Basic model performance report.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00ca94",
   "metadata": {},
   "source": [
    "## 3. Define the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a972c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"An E2E pipeline for BTC-USDT trading: data ingestion, feature engineering, training, and evaluation.\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def trading_pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    gcs_raw_data_uri: str = f\"gs://{BUCKET_NAME}/data/raw/1m_btcusdt_raw.parquet\", # Default path\n",
    "    gcs_prepared_data_base_path: str = f\"gs://{BUCKET_NAME}/data/prepared/\",\n",
    "    gcs_enriched_data_base_path: str = f\"gs://{BUCKET_NAME}/data/enriched/\",\n",
    "    gcs_metrics_base_path: str = f\"gs://{BUCKET_NAME}/evaluation_metrics/\",\n",
    "    training_script_gcs_uri: str = GCS_TRAINING_SCRIPT_PATH,\n",
    "    training_container_uri: str = \"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.0-23:latest\", # Pre-built container for scikit-learn\n",
    "    model_display_name: str = \"btc-usdt-trading-model\",\n",
    "    n_estimators_train: int = 100 # Hyperparameter for training\n",
    "):\n",
    "    # Data Ingestion\n",
    "    data_ingestion_task = data_ingestion_component(\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        gcs_raw_data_uri=gcs_raw_data_uri\n",
    "        # output_prepared_data_uri will be named by KFP, e.g., {gcs_prepared_data_base_path}pipeline_name/run_id/task_name/output_artifact_name\n",
    "    ).set_display_name(\"Data Ingestion\")\n",
    "\n",
    "    # Feature Engineering\n",
    "    feature_engineering_task = feature_engineering_component(\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        prepared_data=data_ingestion_task.outputs[\"output_prepared_data_uri\"]\n",
    "    ).set_display_name(\"Feature Engineering\")\n",
    "\n",
    "    # Model Training (using CustomTrainingJobOp for more control or a custom component that wraps it)\n",
    "    # For simplicity, we'll use a pre-built container and pass the script.\n",
    "    # The model output will be a GCS URI to the directory containing the model artifacts.\n",
    "    # This requires the training_script.py to be accessible, e.g., in GCS.\n",
    "    from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "\n",
    "    # Define where the model artifacts will be stored by the training job\n",
    "    # Vertex AI will create a unique subdirectory here for each training job run\n",
    "    model_output_directory = f\"gs://{bucket_name}/trained_models/{PIPELINE_NAME}/\"\n",
    "\n",
    "    custom_train_job_task = CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        display_name=\"btc-usdt-custom-training\",\n",
    "        script_path=training_script_gcs_uri, # GCS path to your training script\n",
    "        container_uri=training_container_uri, # Or your custom container\n",
    "        requirements=[], # Add if your script needs packages not in the container\n",
    "        model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\", # For deploying scikit-learn models\n",
    "        # Pass arguments to your training script\n",
    "        args=[\n",
    "            f\"--enriched_data_path={feature_engineering_task.outputs['output_enriched_data_uri'].uri}\",\n",
    "            f\"--n_estimators={n_estimators_train}\",\n",
    "            # Add other args your training script expects\n",
    "        ],\n",
    "        # Define the GCS directory where the model should be saved by the training script (AIP_MODEL_DIR)\n",
    "        base_output_directory=model_output_directory,\n",
    "        # Ensure this task runs after feature engineering\n",
    "        # KFP infers this from data passing, but explicit dependency can be set if needed\n",
    "    ).set_display_name(\"Model Training\")\n",
    "    # custom_train_job_task.after(feature_engineering_task) # Usually not needed if outputs are passed\n",
    "\n",
    "    # Model Evaluation\n",
    "    # The model artifact from CustomTrainingJobOp is a GCS path to the directory.\n",
    "    # The evaluation component needs to know how to load the model from this directory.\n",
    "    evaluation_task = model_evaluation_component(\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        enriched_data=feature_engineering_task.outputs[\"output_enriched_data_uri\"],\n",
    "        trained_model=custom_train_job_task.outputs[\"model\"], # This output refers to the GCS directory\n",
    "        gcs_metrics_path=f\"{gcs_metrics_base_path}{PIPELINE_NAME}/{{dsl.PIPELINE_JOB_ID}}/evaluation_metrics.json\"\n",
    "    ).set_display_name(\"Model Evaluation\")\n",
    "\n",
    "    # (Optional) Model Upload and Deployment\n",
    "    # This section can be added if you want to automatically register and deploy the model.\n",
    "    # For now, we'll focus on training and evaluation.\n",
    "\n",
    "    # Example: Upload model to Vertex AI Model Registry\n",
    "    # model_upload_op = ModelUploadOp(\n",
    "    #     project=project_id,\n",
    "    #     display_name=model_display_name,\n",
    "    #     artifact_uri=custom_train_job_task.outputs[\"model\"].uri, # GCS path to model directory\n",
    "    #     serving_container_image_uri=training_container_uri, # Or a specific prediction container\n",
    "    #     # depends_on evaluation_task passing some threshold\n",
    "    # )\n",
    "    # model_upload_op.after(evaluation_task)\n",
    "\n",
    "    # Example: Create Endpoint and Deploy Model\n",
    "    # endpoint_create_op = EndpointCreateOp(\n",
    "    #     project=project_id,\n",
    "    #     display_name=f\"{model_display_name}-endpoint\"\n",
    "    # )\n",
    "    # endpoint_create_op.after(model_upload_op)\n",
    "    # \n",
    "    # model_deploy_op = ModelDeployOp(\n",
    "    #     project=project_id,\n",
    "    #     endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "    #     model=model_upload_op.outputs[\"model\"],\n",
    "    #     deployed_model_display_name=model_display_name,\n",
    "    #     dedicated_resources_machine_type=\"n1-standard-2\", # Choose machine type\n",
    "    #     dedicated_resources_min_replica_count=1,\n",
    "    #     dedicated_resources_max_replica_count=1,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589044ad",
   "metadata": {},
   "source": [
    "## 4. Compile and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON_SPEC_PATH = \"btc_usdt_trading_pipeline.json\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=trading_pipeline,\n",
    "    package_path=PIPELINE_JSON_SPEC_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Pipeline compiled to {PIPELINE_JSON_SPEC_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05df317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "\n",
    "try:\n",
    "    pipeline_job = aiplatform.PipelineJob(\n",
    "        display_name=PIPELINE_NAME,\n",
    "        template_path=PIPELINE_JSON_SPEC_PATH,\n",
    "        pipeline_root=PIPELINE_ROOT, # Redundant if already in pipeline decorator, but good for clarity\n",
    "        # Pass pipeline parameters if they differ from defaults\n",
    "        parameter_values={\n",
    "            'project_id': PROJECT_ID,\n",
    "            'bucket_name': BUCKET_NAME,\n",
    "            # 'gcs_raw_data_uri': 'gs://my-other-bucket/data/raw/1m_btcusdt_raw.parquet', # Example override\n",
    "            'n_estimators_train': 150 # Example override of a training hyperparameter\n",
    "        },\n",
    "        enable_caching=True # Set to False to disable caching for a run\n",
    "    )\n",
    "\n",
    "    pipeline_job.submit()\n",
    "    print(f\"Pipeline job submitted. View in Vertex AI Pipelines: {pipeline_job._dashboard_uri()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error submitting pipeline job: {e}\")\n",
    "    print(\"Please ensure your PROJECT_ID, REGION, and BUCKET_NAME are correctly set, \")\n",
    "    print(\"and that the GCS bucket exists and the Vertex AI API is enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cccd3c",
   "metadata": {},
   "source": [
    "## 5. Next Steps and Refinements\n",
    "\n",
    "1.  **Implement Component Logic:** Replace placeholder logic in `data_ingestion_component`, `feature_engineering_component`, and `model_evaluation_component` with actual code adapted from your existing Python scripts (`fetch_data.py`, `compute_features.py`, `backtest.py`, `model_metrics.py`).\n",
    "2.  **Refine Training Script:** Adapt your `train.py` to be the `training_script.py` used by the `CustomTrainingJobOp`. Ensure it handles GCS paths for input data and model output (using `AIP_MODEL_DIR`).\n",
    "3.  **Custom Containers (If Needed):** If your components or training script have complex dependencies not easily installed via `packages_to_install` or `requirements` in `CustomTrainingJobOp`, you'll need to build custom Docker container images and push them to Google Container Registry (GCR) or Artifact Registry. Then, update the `base_image` for components or `container_uri` for the training job.\n",
    "4.  **Configuration Management:** Integrate your `ConfigManager` or a similar approach for handling configurations (e.g., feature lists, model parameters) within the components, potentially by passing config file paths (in GCS) or specific config values as parameters.\n",
    "5.  **Error Handling and Logging:** Add robust error handling and logging within each component.\n",
    "6.  **Model Registry and Deployment:** Implement the commented-out sections for `ModelUploadOp`, `EndpointCreateOp`, and `ModelDeployOp` if you want to automate model registration and deployment to a Vertex AI Endpoint.\n",
    "7.  **Hyperparameter Tuning:** Vertex AI offers HyperparameterTuningJob which can be integrated into your pipeline before the main training step.\n",
    "8.  **Monitoring:** Set up Vertex AI Model Monitoring for deployed models.\n",
    "9.  **Testing:** Develop tests for your KFP components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9aa19e",
   "metadata": {},
   "source": [
    "## Vertex AI Project Constants and GCS Path Setup\n",
    "Define all essential constants for your Vertex AI project and GCS artifact locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76162ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vertex AI Project Constants ---\n",
    "PROJECT_ID = \"jc-financial-459605\"\n",
    "REGION = \"us-east4\"\n",
    "GCS_BUCKET_URI = \"gs://jc-financial-459605-raw-data-bucket\"\n",
    "\n",
    "# --- GCS Artifact Paths ---\n",
    "CONFIG_GCS_PATH = f\"{GCS_BUCKET_URI}/configs\"\n",
    "RAW_DATA_GCS_PATH = f\"{GCS_BUCKET_URI}/data/raw\"\n",
    "PROCESSED_DATA_GCS_PATH = f\"{GCS_BUCKET_URI}/data/processed\"\n",
    "MODEL_GCS_PATH = f\"{GCS_BUCKET_URI}/models\"\n",
    "PIPELINE_ROOT_GCS_PATH = f\"{GCS_BUCKET_URI}/pipeline_root\"  # For Vertex AI Pipelines artifacts\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "print(f\"GCS_BUCKET_URI: {GCS_BUCKET_URI}\")\n",
    "print(f\"CONFIG_GCS_PATH: {CONFIG_GCS_PATH}\")\n",
    "print(f\"RAW_DATA_GCS_PATH: {RAW_DATA_GCS_PATH}\")\n",
    "print(f\"PROCESSED_DATA_GCS_PATH: {PROCESSED_DATA_GCS_PATH}\")\n",
    "print(f\"MODEL_GCS_PATH: {MODEL_GCS_PATH}\")\n",
    "print(f\"PIPELINE_ROOT_GCS_PATH: {PIPELINE_ROOT_GCS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47507b4f",
   "metadata": {},
   "source": [
    "## Vertex AI SDK Initialization\n",
    "Initialize the Vertex AI SDK with your project, region, and GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=GCS_BUCKET_URI  # You may use PIPELINE_ROOT_GCS_PATH for more specific staging\n",
    ")\n",
    "print(f\"Vertex AI SDK initialized. Project: {PROJECT_ID}, Region: {REGION}, Staging Bucket: {aiplatform.initializer.global_config.staging_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985d9c0",
   "metadata": {},
   "source": [
    "## Google Cloud Storage Client Initialization\n",
    "Set up the GCS client for future storage operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "print(f\"Google Cloud Storage client initialized for project {PROJECT_ID}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798187d2",
   "metadata": {},
   "source": [
    "## Configuration Management from GCS\n",
    "Define a ConfigManager class to load YAML configuration files from GCS and access their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from google.cloud import storage\n",
    "\n",
    "class ConfigManager:\n",
    "    def __init__(self, project_id: str):\n",
    "        self.project_id = project_id\n",
    "        self.storage_client = storage.Client(project=self.project_id)\n",
    "        self.configs = {}\n",
    "\n",
    "    def _download_config_from_gcs(self, gcs_uri: str) -> dict:\n",
    "        \"\"\"Downloads a YAML file from GCS and parses it.\"\"\"\n",
    "        try:\n",
    "            bucket_name = gcs_uri.split('/')[2]\n",
    "            blob_name = '/'.join(gcs_uri.split('/')[3:])\n",
    "\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "\n",
    "            config_string = blob.download_as_text()\n",
    "            config_dict = yaml.safe_load(config_string)\n",
    "            return config_dict\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading or parsing config from {gcs_uri}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def load_config(self, config_name: str, gcs_path: str) -> None:\n",
    "        \"\"\"Loads a specific configuration file from GCS.\"\"\"\n",
    "        print(f\"Loading {config_name} from {gcs_path}...\")\n",
    "        self.configs[config_name] = self._download_config_from_gcs(gcs_path)\n",
    "        if self.configs[config_name]:\n",
    "            print(f\"{config_name} loaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to load {config_name}.\")\n",
    "\n",
    "    def get(self, config_name: str, key: str = None, default=None):\n",
    "        \"\"\"Gets a specific key from a loaded configuration or the entire config.\"\"\"\n",
    "        config_data = self.configs.get(config_name)\n",
    "        if config_data is None:\n",
    "            print(f\"Configuration '{config_name}' not loaded.\")\n",
    "            return default\n",
    "        if key:\n",
    "            # Simple key access, can be expanded for nested keys\n",
    "            return config_data.get(key, default)\n",
    "        return config_data\n",
    "\n",
    "# Example of initializing the ConfigManager\n",
    "config_manager = ConfigManager(project_id=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11992719",
   "metadata": {},
   "source": [
    "### Example: Load and Inspect app_config.yaml from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1df00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Load app_config.yaml from GCS and print sample values\n",
    "app_config_path = f\"{CONFIG_GCS_PATH}/app_config.yaml\"\n",
    "config_mgr = ConfigManager(app_config_path)\n",
    "\n",
    "print(\"Sample config values from app_config.yaml:\")\n",
    "for k, v in list(config_mgr.config.items())[:5]:\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaadfd2",
   "metadata": {},
   "source": [
    "## Integrate Binance Data Fetch KFP Component and Compile Pipeline\n",
    "This section loads the custom KFP component for fetching Binance kline data, defines the pipeline, and compiles it to a JSON spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9cd8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the component\n",
    "import sys\n",
    "component_dir = \"/workspaces/btc_usdt/btc_usdt/btc_usdt_pipeline/components\"\n",
    "if component_dir not in sys.path:\n",
    "    sys.path.append(component_dir)\n",
    "\n",
    "from fetch_binance_data_component import fetch_binance_klines_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15699fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the pipeline\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# Ensure PIPELINE_ROOT is defined in a previous cell\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"btc-usdt-data-ingestion-pipeline\",\n",
    "    description=\"A pipeline to fetch historical BTC/USDT data from Binance.\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def btc_usdt_data_ingestion_pipeline(\n",
    "    symbol: str = \"BTCUSDT\",\n",
    "    interval: str = \"1m\",\n",
    "    start_date: str = \"2023-01-01\",\n",
    "    end_date: str = \"2023-01-02\",\n",
    "    api_url: str = \"https://api.binance.com/api/v3/klines\"\n",
    "):\n",
    "    fetch_data_task = fetch_binance_klines_component(\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        start_date_str=start_date,\n",
    "        end_date_str=end_date,\n",
    "        api_url=api_url,\n",
    "    )\n",
    "    # The output of fetch_data_task (fetch_data_task.outputs['output_data'])\n",
    "    # can be passed to subsequent components if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4661e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled to: /workspaces/btc_usdt/btc_usdt_data_ingestion_pipeline.json\n"
     ]
    }
   ],
   "source": [
    "# 3. Compile the pipeline\n",
    "PIPELINE_JSON_PACKAGE_PATH = \"/workspaces/btc_usdt/btc_usdt_data_ingestion_pipeline.json\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=btc_usdt_data_ingestion_pipeline,\n",
    "    package_path=PIPELINE_JSON_PACKAGE_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Pipeline compiled to: {PIPELINE_JSON_PACKAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622db6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting pipeline job...\n",
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/798634104503/locations/us-east4/pipelineJobs/btc-usdt-data-ingestion-pipeline-20250516164507\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/798634104503/locations/us-east4/pipelineJobs/btc-usdt-data-ingestion-pipeline-20250516164507')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/btc-usdt-data-ingestion-pipeline-20250516164507?project=798634104503\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/798634104503/locations/us-east4/pipelineJobs/btc-usdt-data-ingestion-pipeline-20250516164507')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/btc-usdt-data-ingestion-pipeline-20250516164507?project=798634104503\n",
      "Pipeline job 'btc-usdt-data-ingestion-run-1' submitted. State: 2\n",
      "View the pipeline run in the Vertex AI console: https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/btc-usdt-data-ingestion-pipeline-20250516164507?project=798634104503\n",
      "Pipeline job 'btc-usdt-data-ingestion-run-1' submitted. State: 2\n",
      "View the pipeline run in the Vertex AI console: https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/btc-usdt-data-ingestion-pipeline-20250516164507?project=798634104503\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Ensure PROJECT_ID, REGION, and PIPELINE_ROOT are defined and available\n",
    "# from previous cells in your notebook.\n",
    "\n",
    "# Define a display name for your pipeline run\n",
    "pipeline_display_name = \"btc-usdt-data-ingestion-run-1\" # You can change this for each run\n",
    "\n",
    "# Define any parameters you want to pass to your pipeline.\n",
    "# These will override the default values set in your pipeline definition.\n",
    "# For example, if you want to run it for a different date range or symbol:\n",
    "pipeline_parameters = {\n",
    "    'symbol': 'BTCUSDT',\n",
    "    'interval': '1m',\n",
    "    'start_date': '2023-03-01',\n",
    "    'end_date': '2023-03-05',\n",
    "    'api_url': 'https://api.binance.com/api/v3/klines' # Or your preferred Binance API endpoint\n",
    "}\n",
    "# If you want to use the default parameters defined in the pipeline,\n",
    "# you can pass an empty dictionary: pipeline_parameters = {}\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_display_name,\n",
    "    template_path=\"/workspaces/btc_usdt/btc_usdt_data_ingestion_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,  # This should be the same PIPELINE_ROOT used during compilation\n",
    "    parameter_values=pipeline_parameters,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    # enable_caching=True # Optional: Set to True to enable caching for pipeline steps\n",
    ")\n",
    "\n",
    "print(\"Submitting pipeline job...\")\n",
    "job.submit()\n",
    "\n",
    "print(f\"Pipeline job '{job.display_name}' submitted. State: {job.state}\")\n",
    "print(f\"View the pipeline run in the Vertex AI console: {job._dashboard_uri()}\")\n",
    "# You can also use job.wait() if you want the notebook to block until the pipeline completes.\n",
    "# job.wait()\n",
    "# print(\"Pipeline job finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb64205",
   "metadata": {},
   "source": [
    "# (Optional) Submitting the pipeline job\n",
    "# You can submit and run the pipeline using aiplatform.PipelineJob in a later step.\n",
    "# For now, just compiling is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7486a2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal pipeline compiled to: /workspaces/btc_usdt/minimal_hello_world_pipeline.json\n",
      "Submitting minimal pipeline job...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/798634104503/locations/us-east4/pipelineJobs/minimal-hello-world-pipeline-20250516164135\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/798634104503/locations/us-east4/pipelineJobs/minimal-hello-world-pipeline-20250516164135')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/minimal-hello-world-pipeline-20250516164135?project=798634104503\n",
      "PipelineJob created. Resource name: projects/798634104503/locations/us-east4/pipelineJobs/minimal-hello-world-pipeline-20250516164135\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/798634104503/locations/us-east4/pipelineJobs/minimal-hello-world-pipeline-20250516164135')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/minimal-hello-world-pipeline-20250516164135?project=798634104503\n",
      "Minimal pipeline job 'minimal-hello-world-run-1' submitted. State: 2\n",
      "View the minimal pipeline run in the Vertex AI console: https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/minimal-hello-world-pipeline-20250516164135?project=798634104503\n",
      "Minimal pipeline job 'minimal-hello-world-run-1' submitted. State: 2\n",
      "View the minimal pipeline run in the Vertex AI console: https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/minimal-hello-world-pipeline-20250516164135?project=798634104503\n"
     ]
    }
   ],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# 1. Define a very simple component\n",
    "@dsl.component(base_image=\"python:3.9-slim\")\n",
    "def hello_world_component(text: str) -> str:\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "# 2. Define a minimal pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"minimal-hello-world-pipeline\",\n",
    "    description=\"A very simple pipeline to test basic functionality.\",\n",
    "    pipeline_root=PIPELINE_ROOT, # PIPELINE_ROOT should be defined from previous cells\n",
    ")\n",
    "def minimal_pipeline(\n",
    "    greeting: str = \"Hello, Vertex AI Pipelines!\"\n",
    "):\n",
    "    hello_task = hello_world_component(text=greeting)\n",
    "\n",
    "# 3. Compile the minimal pipeline\n",
    "MINIMAL_PIPELINE_JSON_PACKAGE_PATH = \"/workspaces/btc_usdt/minimal_hello_world_pipeline.json\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=minimal_pipeline,\n",
    "    package_path=MINIMAL_PIPELINE_JSON_PACKAGE_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Minimal pipeline compiled to: {MINIMAL_PIPELINE_JSON_PACKAGE_PATH}\")\n",
    "\n",
    "# 4. Submit and run the minimal pipeline\n",
    "minimal_pipeline_display_name = \"minimal-hello-world-run-1\"\n",
    "\n",
    "minimal_job = aiplatform.PipelineJob(\n",
    "    display_name=minimal_pipeline_display_name,\n",
    "    template_path=MINIMAL_PIPELINE_JSON_PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'greeting': 'Testing minimal execution'\n",
    "    },\n",
    "    project=PROJECT_ID, # PROJECT_ID should be defined from previous cells\n",
    "    location=REGION,    # REGION should be defined from previous cells\n",
    ")\n",
    "\n",
    "print(\"Submitting minimal pipeline job...\")\n",
    "minimal_job.submit()\n",
    "\n",
    "print(f\"Minimal pipeline job '{minimal_job.display_name}' submitted. State: {minimal_job.state}\")\n",
    "print(f\"View the minimal pipeline run in the Vertex AI console: {minimal_job._dashboard_uri()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
